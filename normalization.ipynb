{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92895339-8e11-4208-a216-8b15f6933f62",
      "metadata": {
        "id": "92895339-8e11-4208-a216-8b15f6933f62"
      },
      "source": [
        "# Normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I implement weight norm, batch norm and layer norm.\n",
        "\n",
        "[Weight normalization](https://arxiv.org/pdf/1602.07868.pdf) is a method of reparameterizing the weight vector `w` in the standard linear neural network layer\n",
        "\n",
        "```\n",
        "y = φ(w · x + b)\n",
        "```\n",
        "\n",
        " as\n",
        "\n",
        "```\n",
        "w = (v*g)/(||v||)\n",
        "```\n",
        "where `v` is the learnable weight vector and `g` is a learnable scalar. This reparameterization has the effect of decoupling the magnitude and the direction of the weight vector.\n",
        "\n",
        "Both [layer](https://arxiv.org/abs/1607.06450) and [batch](https://arxiv.org/abs/1502.03167) normalization are methods of reducing covariate shift (change in input distribution) by normalizing the input to each layer. They involve normalizing the input\n",
        "\n",
        "```\n",
        "input_normalized = (input-input_mean)/input_std\n",
        "```\n",
        "\n",
        "which is then transformed using learnable parameters a and b\n",
        "\n",
        "```\n",
        "final_input = a * input_normalized + b\n",
        "```\n",
        "This results in each layer in the network receiving input with a consistent distribution, allowing it to train faster and become more accurate.\n",
        "\n",
        "The difference between batch norm and layer norm lies in how `input_mean` and `input_std` are calculated. In batch norm, they are typically calculated across each minibatch, while in layer norm they are calculated individually for each input vector to each layer.\n",
        "\n",
        "Layer norm was created to remove the relationship between batch size and the effect of normalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MRoe3XIyK-o6"
      },
      "id": "MRoe3XIyK-o6"
    },
    {
      "cell_type": "markdown",
      "id": "069df629-a42c-4f1e-8be0-436358691b04",
      "metadata": {
        "id": "069df629-a42c-4f1e-8be0-436358691b04"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "553610b4-d2b7-4d4b-8f79-faf4d5a9ee88",
      "metadata": {
        "id": "553610b4-d2b7-4d4b-8f79-faf4d5a9ee88"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "\n",
        "import time\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c2bc754-b75f-45ce-afdf-ac8b8c40c0c6",
      "metadata": {
        "id": "4c2bc754-b75f-45ce-afdf-ac8b8c40c0c6"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "01ca06b6-c0a2-4df5-8996-47b3759470a1",
      "metadata": {
        "id": "01ca06b6-c0a2-4df5-8996-47b3759470a1"
      },
      "outputs": [],
      "source": [
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "train_dataloader = DataLoader(training_data, batch_size=128, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c1b55c-80fe-4487-84f2-fe846da66518",
      "metadata": {
        "id": "93c1b55c-80fe-4487-84f2-fe846da66518"
      },
      "source": [
        "## Weight Norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is an edited version of PyTorch's `Linear` layer with weight normalization added."
      ],
      "metadata": {
        "id": "zWi0Dhyg_IK3"
      },
      "id": "zWi0Dhyg_IK3"
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightNormalizedLinear(nn.Module):\n",
        "    __constants__ = ['in_features', 'out_features']\n",
        "    in_features: int\n",
        "    out_features: int\n",
        "    weight: Tensor\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.g = Parameter(torch.empty((out_features,in_features),**factory_kwargs))\n",
        "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.g, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return F.linear(input, self.g*(self.weight)/torch.norm(self.weight,dim=0), self.bias)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )"
      ],
      "metadata": {
        "id": "2lXYZ14jyHsa"
      },
      "id": "2lXYZ14jyHsa",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Norm"
      ],
      "metadata": {
        "id": "w0Fqa08Pdo66"
      },
      "id": "w0Fqa08Pdo66"
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(BatchNorm, self).__init__()\n",
        "        self.beta = Parameter(torch.empty(1,device=device))\n",
        "        self.gamma = Parameter(torch.empty(1,device=device))\n",
        "        torch.nn.init.uniform_(self.beta)\n",
        "        torch.nn.init.uniform_(self.gamma)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = torch.mean(x)\n",
        "    diff = x - mean\n",
        "    stdev = torch.mean(diff*diff)\n",
        "    z = (x-mean)/torch.sqrt(stdev+0.000001)\n",
        "    return self.gamma*z + self.beta"
      ],
      "metadata": {
        "id": "YpnKQ-7Sdufn"
      },
      "id": "YpnKQ-7Sdufn",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Norm"
      ],
      "metadata": {
        "id": "LNrprnRndx08"
      },
      "id": "LNrprnRndx08"
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.beta = Parameter(torch.empty(1,device=device))\n",
        "        self.gamma = Parameter(torch.empty(1,device=device))\n",
        "        torch.nn.init.uniform_(self.beta)\n",
        "        torch.nn.init.uniform_(self.gamma)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = torch.mean(x,dim=1)\n",
        "    diff = x - mean.reshape(-1,1)\n",
        "    stdev = torch.mean(diff*diff,dim=1)\n",
        "    z = (diff)/torch.sqrt(stdev.reshape(-1,1)+0.000001)\n",
        "    return self.gamma*z + self.beta"
      ],
      "metadata": {
        "id": "RpKzLfJGd0Y7"
      },
      "id": "RpKzLfJGd0Y7",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "655QIw9s09kx"
      },
      "id": "655QIw9s09kx"
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.p1 = WeightNormalizedLinear(784,512,device=device)\n",
        "        self.ln1 = LayerNorm()\n",
        "        self.p2 = WeightNormalizedLinear(512,512,device=device)\n",
        "        self.bn2 = BatchNorm()\n",
        "        self.p3 = WeightNormalizedLinear(512,256,device=device)\n",
        "        self.ln3 = LayerNorm()\n",
        "        self.p4 = WeightNormalizedLinear(256,10,device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.p1(x))\n",
        "        x = self.ln1(x)\n",
        "        x = F.relu(self.p2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(self.p3(x))\n",
        "        x = self.ln3(x)\n",
        "        x = self.p4(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_3n2S1_T1MaY"
      },
      "id": "_3n2S1_T1MaY",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(),lr = 0.001)"
      ],
      "metadata": {
        "id": "M7I6_U9z3OLw"
      },
      "id": "M7I6_U9z3OLw",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Test"
      ],
      "metadata": {
        "id": "ZAxMXNYgoEtw"
      },
      "id": "ZAxMXNYgoEtw"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.to(device)\n",
        "    for inputs, labels in train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model.forward(inputs.to(device).reshape(-1,784))\n",
        "      loss = loss_fn(outputs, labels.to(device))\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "metadata": {
        "id": "pAC2UT0uoD04"
      },
      "id": "pAC2UT0uoD04",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1d87240e-033f-4e44-b4a1-f51d1a39e946",
      "metadata": {
        "id": "1d87240e-033f-4e44-b4a1-f51d1a39e946"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader = test_dataloader):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model.forward(inputs.to(device).reshape(-1,784))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.to(device)).sum().item()\n",
        "    return correct/total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "  train_one_epoch(model)\n",
        "  test_acc = test(model)\n",
        "  print(f\"epoch : {epoch+1}, test acc : {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM313KLl1tH7",
        "outputId": "dda17fed-6169-472a-ec25-5485b9789263"
      },
      "id": "YM313KLl1tH7",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1, test acc : 0.9806\n",
            "epoch : 2, test acc : 0.9805\n",
            "epoch : 3, test acc : 0.9815\n",
            "epoch : 4, test acc : 0.9799\n",
            "epoch : 5, test acc : 0.9814\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "069df629-a42c-4f1e-8be0-436358691b04",
        "4c2bc754-b75f-45ce-afdf-ac8b8c40c0c6"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}