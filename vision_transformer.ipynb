{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92895339-8e11-4208-a216-8b15f6933f62",
      "metadata": {
        "id": "92895339-8e11-4208-a216-8b15f6933f62"
      },
      "source": [
        "# Vision Transformer Cifar-10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I implement a vision transformer, as described in this [paper](https://arxiv.org/pdf/2010.11929v2.pdf). Transformers have been widely used in NLP but can also be used in computer vision tasks, where they typically achieve better performance than convolution based models on very large datasets while CNNs win out on smaller datasets.\n",
        "\n",
        "I wasn't able to fully train the model in this notebook as I am running out of google colab credits for this month. It seemed to perform well on the initial couple of epochs and a different model trained up to ~70% accuracy before I had to restart. I am interested to see how it performs when I am able to test it and see if it is able to beat the ResNet model I implemented on the Cifar dataset."
      ],
      "metadata": {
        "id": "8ssyC2l-GpYJ"
      },
      "id": "8ssyC2l-GpYJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Untitled.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "X6aPg9Svp2O_"
      },
      "id": "X6aPg9Svp2O_"
    },
    {
      "cell_type": "markdown",
      "id": "069df629-a42c-4f1e-8be0-436358691b04",
      "metadata": {
        "id": "069df629-a42c-4f1e-8be0-436358691b04"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553610b4-d2b7-4d4b-8f79-faf4d5a9ee88",
      "metadata": {
        "id": "553610b4-d2b7-4d4b-8f79-faf4d5a9ee88"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import math\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xYOcdrM345U",
        "outputId": "2d028c36-11df-4021-b02a-301fa39a56df"
      },
      "id": "1xYOcdrM345U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c2bc754-b75f-45ce-afdf-ac8b8c40c0c6",
      "metadata": {
        "id": "4c2bc754-b75f-45ce-afdf-ac8b8c40c0c6"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01ca06b6-c0a2-4df5-8996-47b3759470a1",
      "metadata": {
        "id": "01ca06b6-c0a2-4df5-8996-47b3759470a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1a82df-d097-448a-f09f-f3558be794be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#I took data loader, visualisation and test code from here\n",
        "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     #transforms.Lambda(nn.ZeroPad2d(4)),\n",
        "     #transforms.RandomCrop(32),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "batch_size = 128\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1, drop_last = True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1, drop_last = True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "mX1wFwW4o3P-"
      },
      "id": "mX1wFwW4o3P-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This embedding turns an image `(batch_size,channels,height,width)` into `(batch_size,patches+1,patch_height*patch_width*patch_channels)`, where patches+1 in the second dimension is due to the addition of the start token"
      ],
      "metadata": {
        "id": "LnYPC46Ho5Mp"
      },
      "id": "LnYPC46Ho5Mp"
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self,d_model):\n",
        "      super(Embedding, self).__init__()\n",
        "      self.linear = nn.Linear(48,d_model,device=device)\n",
        "      #learnable start token\n",
        "      #we append this token to the start of every input to model and then we use this token position as model output\n",
        "      self.new_token = nn.Parameter(torch.empty((d_model),device=device))\n",
        "    def forward(self,x):\n",
        "      #get patches of image (kind of similar to a how a convolution works)\n",
        "      #assertions are commented out for efficiency\n",
        "      x = x.unfold(-1,4,4).unfold(-3,4,4).flatten(-4,-3).permute(0,2,1,3,4).flatten(-3)\n",
        "      x = F.relu(self.linear(x))\n",
        "      sh = list(x.shape)\n",
        "      sh[-2]=1\n",
        "      #test_tensor = x.clone()\n",
        "      new_token_stack = self.new_token.expand(sh)\n",
        "      x = torch.cat((new_token_stack,x),dim=-2)\n",
        "      #ensure everything is working correctly\n",
        "      #assert torch.sum(x[:,0]-self.new_token) == 0\n",
        "      #assert torch.sum(test_tensor-x[:,1:,:])==0\n",
        "      return x\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "model = Embedding(512)\n",
        "print(images.shape)\n",
        "print(model(images.to(device)).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulRzR6W-x9UR",
        "outputId": "47da80b1-8fac-4dd2-f5e5-88701ff3f44c"
      },
      "id": "ulRzR6W-x9UR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 32, 32])\n",
            "torch.Size([128, 65, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding (1d)"
      ],
      "metadata": {
        "id": "VLmMmP3toq0p"
      },
      "id": "VLmMmP3toq0p"
    },
    {
      "cell_type": "code",
      "source": [
        "#copied this function from the annotated transformer\n",
        "#https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "oinsbUAxLXun"
      },
      "id": "oinsbUAxLXun",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block"
      ],
      "metadata": {
        "id": "dYhrPpLuiabp"
      },
      "id": "dYhrPpLuiabp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each `Block` consists of an attention layer and a fully connected layer. Each layer is followed by a residual connection such that they can each be described by `y = F(x) + x`. The output from this layer is then normalised using layer normalization."
      ],
      "metadata": {
        "id": "R6Sted5fecr5"
      },
      "id": "R6Sted5fecr5"
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,d_model,dropout):\n",
        "    super(Block, self).__init__()\n",
        "    self.mha = nn.MultiheadAttention(d_model, 4,batch_first=True,dropout=dropout)\n",
        "    self.ln1 = nn.LayerNorm([65,d_model])\n",
        "    self.fc2 = nn.Linear(d_model,3072)\n",
        "    self.fc3 = nn.Linear(3072, d_model)\n",
        "    self.ln2 = nn.LayerNorm([65,d_model])\n",
        "\n",
        "  def forward(self,x):\n",
        "    res = x.clone()\n",
        "    x = self.mha(x,x,x)[0]\n",
        "    x = self.ln1(x+res)\n",
        "    res = x.clone()\n",
        "    x = F.gelu(self.fc2(x))\n",
        "    x = self.ln2(self.fc3(x)+res)\n",
        "    return x"
      ],
      "metadata": {
        "id": "e-p7ky5TG8pi"
      },
      "id": "e-p7ky5TG8pi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "93c1b55c-80fe-4487-84f2-fe846da66518",
      "metadata": {
        "id": "93c1b55c-80fe-4487-84f2-fe846da66518"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1583a309-390c-4438-8d5c-54fcb38c75b4",
      "metadata": {
        "id": "1583a309-390c-4438-8d5c-54fcb38c75b4"
      },
      "outputs": [],
      "source": [
        "class AttentionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttentionNet, self).__init__()\n",
        "        d_model = 768\n",
        "        dropout = .1\n",
        "\n",
        "        self.b1 = Block(d_model,dropout)\n",
        "        self.b2 = Block(d_model,dropout)\n",
        "        self.b3 = Block(d_model,dropout)\n",
        "        self.b4 = Block(d_model,dropout)\n",
        "        self.b5 = Block(d_model,dropout)\n",
        "        self.b6 = Block(d_model,dropout)\n",
        "\n",
        "        self.emb = Embedding(d_model)\n",
        "        self.pos = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        self.fc3 = nn.Linear(d_model,d_model)\n",
        "        self.fc4 = nn.Linear(d_model,d_model)\n",
        "        self.fc5 = nn.Linear(d_model,256)\n",
        "        self.fc6 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.emb(x)\n",
        "        x = self.pos(x)\n",
        "\n",
        "        x = self.b1(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.b4(x)\n",
        "        x = self.b5(x)\n",
        "        x = self.b6(x)\n",
        "\n",
        "        x = x[:,0]\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4643ab58-a7a6-4a02-bf1a-c26dd3d5993a",
      "metadata": {
        "id": "4643ab58-a7a6-4a02-bf1a-c26dd3d5993a"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionNet().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "PWIbcKrvG_fC"
      },
      "id": "PWIbcKrvG_fC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see below that the model does train. It is slower to train than the ResNet20 model I implemented in another notebook, due to its lower inductive bias (assumptions made during creation of the model), but this is also what should allow it to be more accurate when it is trained.\n",
        "\n",
        "I haven't trained it for long as I am running low on Google Colab credits, but when I build up some credits, I will train the model for longer. I am interested to see how it performs relative to convolution based models."
      ],
      "metadata": {
        "id": "XJ84wTv4nFDR"
      },
      "id": "XJ84wTv4nFDR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54afdccc-177c-4f92-9e46-4a3c1bbed3ac",
      "metadata": {
        "id": "54afdccc-177c-4f92-9e46-4a3c1bbed3ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "1cdd64af-c809-468b-f2ae-eed12891f8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23707932692307693\n",
            "[0.029987492015138072]\n",
            "0.2631209935897436\n",
            "[0.02994998892051045]\n",
            "0.2916666666666667\n",
            "[0.029887553261202383]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6bbf84fa7459>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mtest_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6bbf84fa7459>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "learning_rate=.03\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum = .9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,30000)\n",
        "\n",
        "def test_():\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = model(inputs.to(device))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.to(device)).sum().item()\n",
        "    print(correct/total)\n",
        "    return correct/total\n",
        "\n",
        "def train_one_epoch():\n",
        "    for inputs, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = loss_fn(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "for x in range(100):\n",
        "  train_one_epoch()\n",
        "  test_()\n",
        "  print(scheduler.get_last_lr())\n",
        "  torch.save(model.state_dict(), f'/content/drive/MyDrive/vision_transformer_{x}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}